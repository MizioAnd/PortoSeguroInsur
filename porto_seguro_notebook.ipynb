{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# porto_seguro_insur.py\n",
    "#  Assumes python vers. 3.6\n",
    "# __author__ = 'mizio'\n",
    "\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "from fancyimpute import MICE\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PortoSeguroInsur:\n",
    "    def __init__(self):\n",
    "        self.df = PortoSeguroInsur.df\n",
    "        self.df_test = PortoSeguroInsur.df_test\n",
    "        self.df_submission = PortoSeguroInsur.df_submission\n",
    "        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n",
    "\n",
    "\n",
    "    # Load data into Pandas DataFrame\n",
    "    # For .read_csv, always use header=0 when you know row 0 is the header row\n",
    "    df = pd.read_csv('../input/train.csv', header=0)\n",
    "    df_test = pd.read_csv('../input/test.csv', header=0)\n",
    "    df_submission = pd.read_csv('../input/sample_submission.csv', header=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def features_with_null_logical(df, axis=1):\n",
    "        row_length = len(df._get_axis(0))\n",
    "        # Axis to count non null values in. aggregate_axis=0 implies counting for every feature\n",
    "        aggregate_axis = 1 - axis\n",
    "        features_non_null_series = df.count(axis=aggregate_axis)\n",
    "        # Whenever count() differs from row_length it implies a null value exists in feature column and a False in mask\n",
    "        mask = row_length == features_non_null_series\n",
    "        return mask\n",
    "\n",
    "    def missing_values_in_dataframe(self, df):\n",
    "        mask = self.features_with_null_logical(df)\n",
    "        print(df[mask[mask == 0].index.values].isnull().sum())\n",
    "        print('\\n')\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_numerical_features(df):\n",
    "        df = df.copy()\n",
    "        df = df.copy()\n",
    "        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n",
    "            df) == 0)]\n",
    "        return non_numerical_feature_names\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_non_numerical_features(df):\n",
    "        df = df.copy()\n",
    "        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n",
    "            df))]\n",
    "        return non_numerical_feature_names\n",
    "\n",
    "    @staticmethod\n",
    "    def numerical_feature_logical_incl_hidden_num(df):\n",
    "        logical_of_non_numeric_features = np.zeros(df.columns.shape[0], dtype=int)\n",
    "        for ite in np.arange(0, df.columns.shape[0]):\n",
    "            try:\n",
    "                str(df[df.columns[ite]][0]) + df[df.columns[ite]][0]\n",
    "                logical_of_non_numeric_features[ite] = True\n",
    "            except TypeError:\n",
    "                hej = 'Oops'\n",
    "        return logical_of_non_numeric_features\n",
    "\n",
    "    def clean_data(self, df, is_train_data=1):\n",
    "        df = df.copy()\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            if is_train_data:\n",
    "                df = df.dropna()\n",
    "            else:\n",
    "                df = df.dropna(1)\n",
    "        return df\n",
    "\n",
    "    def reformat_data(self, labels, num_labels):\n",
    "        # Map labels/target value to one-hot-encoded frame. None is same as implying newaxis() just replicating array\n",
    "        # if num_labels > 2:\n",
    "        labels = (np.arange(num_labels) == labels[:, None]).astype(np.float64)\n",
    "        return labels\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        # Sum the number of cases where the predictions are correct and divide by the number of predictions\n",
    "        number_of_correct_predictions = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "        return 100*number_of_correct_predictions/predictions.shape[0]\n",
    "\n",
    "    def linear_model(self, input_vector, weight_matrix, bias_vector):\n",
    "        # f(x) = Wx + b\n",
    "        # W is the weight matrix with elements w_ij\n",
    "        # x is the input vector\n",
    "        # b is the bias vector\n",
    "        # In the machine learning literature f(x) is called an activation\n",
    "        return tf.matmul(input_vector, weight_matrix) + bias_vector\n",
    "\n",
    "    def activation_out(self, logit):\n",
    "        return self.activation(logit, switch_var=0)\n",
    "\n",
    "    def activation_hidden(self, logit):\n",
    "        return self.activation(logit, switch_var=0)\n",
    "\n",
    "    def activation(self, logit, switch_var=0):\n",
    "        # Also called the activation function\n",
    "        if switch_var == 0:\n",
    "            # Logistic sigmoid function.\n",
    "            # sigma(a) = 1/(1+exp(-a))\n",
    "            return tf.nn.sigmoid(logit)\n",
    "        elif switch_var == 1:\n",
    "            # Using Rectifier as activation function. Rectified linear unit (ReLU). Compared to sigmoid or other\n",
    "            # activation functions it allows for faster and effective training of neural architectures.\n",
    "            # f(x) = max(x,0)\n",
    "            return tf.nn.relu(logit)\n",
    "        else:\n",
    "            # Softmax function.\n",
    "            # S(y_i) = e^y_i/(Sum_j e^y_j)\n",
    "            return tf.nn.softmax(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 59)\n",
      "(892816, 58)\n",
      "df_test.shape: (892816, 46)\n",
      "After dropping NaN\n",
      "(124931, 59)\n",
      "(892816, 46)\n"
     ]
    }
   ],
   "source": [
    "porto_seguro_insur = PortoSeguroInsur()\n",
    "df = porto_seguro_insur.df.copy()\n",
    "df_test = porto_seguro_insur.df_test.copy()\n",
    "df_submission = porto_seguro_insur.df_submission.copy()\n",
    "\n",
    "df = df.replace(-1, np.NaN)\n",
    "df_test = df_test.replace(-1, np.NaN)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "# Clean data for NaN\n",
    "df = porto_seguro_insur.clean_data(df)\n",
    "df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\n",
    "print('df_test.shape: %s' % str(df_test.shape))  # (892816, 46)\n",
    "# df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\n",
    "id_df_test = df_test['id']  # Submission column\n",
    "print(\"After dropping NaN\")\n",
    "print(df.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAINING DATA:----------------------------------------------- \n",
      "\n",
      "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
      "3  16       0          0            1.0          2            0.0   \n",
      "7  22       0          5            1.0          4            0.0   \n",
      "9  28       1          1            1.0          2            0.0   \n",
      "\n",
      "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin       ...        \\\n",
      "3            0.0              1              0              0       ...         \n",
      "7            0.0              1              0              0       ...         \n",
      "9            0.0              0              1              0       ...         \n",
      "\n",
      "   ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
      "3           2           2           4           9               0   \n",
      "7           7           1           3           6               1   \n",
      "9           3           5           0           6               0   \n",
      "\n",
      "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
      "3               0               0               0               0   \n",
      "7               0               1               0               1   \n",
      "9               1               0               0               1   \n",
      "\n",
      "   ps_calc_20_bin  \n",
      "3               0  \n",
      "7               0  \n",
      "9               0  \n",
      "\n",
      "[3 rows x 59 columns]\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 124931 entries, 3 to 595205\n",
      "Data columns (total 59 columns):\n",
      "id                124931 non-null int64\n",
      "target            124931 non-null int64\n",
      "ps_ind_01         124931 non-null int64\n",
      "ps_ind_02_cat     124931 non-null float64\n",
      "ps_ind_03         124931 non-null int64\n",
      "ps_ind_04_cat     124931 non-null float64\n",
      "ps_ind_05_cat     124931 non-null float64\n",
      "ps_ind_06_bin     124931 non-null int64\n",
      "ps_ind_07_bin     124931 non-null int64\n",
      "ps_ind_08_bin     124931 non-null int64\n",
      "ps_ind_09_bin     124931 non-null int64\n",
      "ps_ind_10_bin     124931 non-null int64\n",
      "ps_ind_11_bin     124931 non-null int64\n",
      "ps_ind_12_bin     124931 non-null int64\n",
      "ps_ind_13_bin     124931 non-null int64\n",
      "ps_ind_14         124931 non-null int64\n",
      "ps_ind_15         124931 non-null int64\n",
      "ps_ind_16_bin     124931 non-null int64\n",
      "ps_ind_17_bin     124931 non-null int64\n",
      "ps_ind_18_bin     124931 non-null int64\n",
      "ps_reg_01         124931 non-null float64\n",
      "ps_reg_02         124931 non-null float64\n",
      "ps_reg_03         124931 non-null float64\n",
      "ps_car_01_cat     124931 non-null float64\n",
      "ps_car_02_cat     124931 non-null float64\n",
      "ps_car_03_cat     124931 non-null float64\n",
      "ps_car_04_cat     124931 non-null int64\n",
      "ps_car_05_cat     124931 non-null float64\n",
      "ps_car_06_cat     124931 non-null int64\n",
      "ps_car_07_cat     124931 non-null float64\n",
      "ps_car_08_cat     124931 non-null int64\n",
      "ps_car_09_cat     124931 non-null float64\n",
      "ps_car_10_cat     124931 non-null int64\n",
      "ps_car_11_cat     124931 non-null int64\n",
      "ps_car_11         124931 non-null float64\n",
      "ps_car_12         124931 non-null float64\n",
      "ps_car_13         124931 non-null float64\n",
      "ps_car_14         124931 non-null float64\n",
      "ps_car_15         124931 non-null float64\n",
      "ps_calc_01        124931 non-null float64\n",
      "ps_calc_02        124931 non-null float64\n",
      "ps_calc_03        124931 non-null float64\n",
      "ps_calc_04        124931 non-null int64\n",
      "ps_calc_05        124931 non-null int64\n",
      "ps_calc_06        124931 non-null int64\n",
      "ps_calc_07        124931 non-null int64\n",
      "ps_calc_08        124931 non-null int64\n",
      "ps_calc_09        124931 non-null int64\n",
      "ps_calc_10        124931 non-null int64\n",
      "ps_calc_11        124931 non-null int64\n",
      "ps_calc_12        124931 non-null int64\n",
      "ps_calc_13        124931 non-null int64\n",
      "ps_calc_14        124931 non-null int64\n",
      "ps_calc_15_bin    124931 non-null int64\n",
      "ps_calc_16_bin    124931 non-null int64\n",
      "ps_calc_17_bin    124931 non-null int64\n",
      "ps_calc_18_bin    124931 non-null int64\n",
      "ps_calc_19_bin    124931 non-null int64\n",
      "ps_calc_20_bin    124931 non-null int64\n",
      "dtypes: float64(20), int64(39)\n",
      "memory usage: 57.2 MB\n",
      "None\n",
      "\n",
      "\n",
      "                 id         target      ps_ind_01  ps_ind_02_cat  \\\n",
      "count  1.249310e+05  124931.000000  124931.000000  124931.000000   \n",
      "mean   7.424903e+05       0.045385       2.056191       1.353507   \n",
      "std    4.295754e+05       0.208148       2.102161       0.691822   \n",
      "min    1.600000e+01       0.000000       0.000000       1.000000   \n",
      "25%    3.695480e+05       0.000000       0.000000       1.000000   \n",
      "50%    7.422660e+05       0.000000       1.000000       1.000000   \n",
      "75%    1.113844e+06       0.000000       4.000000       2.000000   \n",
      "max    1.488009e+06       1.000000       7.000000       4.000000   \n",
      "\n",
      "           ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  ps_ind_06_bin  \\\n",
      "count  124931.000000  124931.000000  124931.000000  124931.000000   \n",
      "mean        4.922613       0.442452       0.393569       0.355836   \n",
      "std         2.737705       0.496679       1.265840       0.478768   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         3.000000       0.000000       0.000000       0.000000   \n",
      "50%         5.000000       0.000000       0.000000       0.000000   \n",
      "75%         7.000000       1.000000       0.000000       1.000000   \n",
      "max        11.000000       1.000000       6.000000       1.000000   \n",
      "\n",
      "       ps_ind_07_bin  ps_ind_08_bin       ...           ps_calc_11  \\\n",
      "count  124931.000000  124931.000000       ...        124931.000000   \n",
      "mean        0.285910       0.189449       ...             5.435432   \n",
      "std         0.451848       0.391866       ...             2.330085   \n",
      "min         0.000000       0.000000       ...             0.000000   \n",
      "25%         0.000000       0.000000       ...             4.000000   \n",
      "50%         0.000000       0.000000       ...             5.000000   \n",
      "75%         1.000000       0.000000       ...             7.000000   \n",
      "max         1.000000       1.000000       ...            18.000000   \n",
      "\n",
      "          ps_calc_12     ps_calc_13     ps_calc_14  ps_calc_15_bin  \\\n",
      "count  124931.000000  124931.000000  124931.000000   124931.000000   \n",
      "mean        1.434768       2.875147       7.545165        0.123020   \n",
      "std         1.202838       1.699413       2.740946        0.328461   \n",
      "min         0.000000       0.000000       0.000000        0.000000   \n",
      "25%         1.000000       2.000000       6.000000        0.000000   \n",
      "50%         1.000000       3.000000       7.000000        0.000000   \n",
      "75%         2.000000       4.000000       9.000000        0.000000   \n",
      "max        10.000000      13.000000      22.000000        1.000000   \n",
      "\n",
      "       ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
      "count   124931.000000   124931.000000   124931.000000   124931.000000   \n",
      "mean         0.627498        0.553385        0.286126        0.350249   \n",
      "std          0.483473        0.497144        0.451951        0.477050   \n",
      "min          0.000000        0.000000        0.000000        0.000000   \n",
      "25%          0.000000        0.000000        0.000000        0.000000   \n",
      "50%          1.000000        1.000000        0.000000        0.000000   \n",
      "75%          1.000000        1.000000        1.000000        1.000000   \n",
      "max          1.000000        1.000000        1.000000        1.000000   \n",
      "\n",
      "       ps_calc_20_bin  \n",
      "count   124931.000000  \n",
      "mean         0.153581  \n",
      "std          0.360548  \n",
      "min          0.000000  \n",
      "25%          0.000000  \n",
      "50%          0.000000  \n",
      "75%          0.000000  \n",
      "max          1.000000  \n",
      "\n",
      "[8 rows x 59 columns]\n",
      "\n",
      "\n",
      "id                  int64\n",
      "target              int64\n",
      "ps_ind_01           int64\n",
      "ps_ind_02_cat     float64\n",
      "ps_ind_03           int64\n",
      "ps_ind_04_cat     float64\n",
      "ps_ind_05_cat     float64\n",
      "ps_ind_06_bin       int64\n",
      "ps_ind_07_bin       int64\n",
      "ps_ind_08_bin       int64\n",
      "ps_ind_09_bin       int64\n",
      "ps_ind_10_bin       int64\n",
      "ps_ind_11_bin       int64\n",
      "ps_ind_12_bin       int64\n",
      "ps_ind_13_bin       int64\n",
      "ps_ind_14           int64\n",
      "ps_ind_15           int64\n",
      "ps_ind_16_bin       int64\n",
      "ps_ind_17_bin       int64\n",
      "ps_ind_18_bin       int64\n",
      "ps_reg_01         float64\n",
      "ps_reg_02         float64\n",
      "ps_reg_03         float64\n",
      "ps_car_01_cat     float64\n",
      "ps_car_02_cat     float64\n",
      "ps_car_03_cat     float64\n",
      "ps_car_04_cat       int64\n",
      "ps_car_05_cat     float64\n",
      "ps_car_06_cat       int64\n",
      "ps_car_07_cat     float64\n",
      "ps_car_08_cat       int64\n",
      "ps_car_09_cat     float64\n",
      "ps_car_10_cat       int64\n",
      "ps_car_11_cat       int64\n",
      "ps_car_11         float64\n",
      "ps_car_12         float64\n",
      "ps_car_13         float64\n",
      "ps_car_14         float64\n",
      "ps_car_15         float64\n",
      "ps_calc_01        float64\n",
      "ps_calc_02        float64\n",
      "ps_calc_03        float64\n",
      "ps_calc_04          int64\n",
      "ps_calc_05          int64\n",
      "ps_calc_06          int64\n",
      "ps_calc_07          int64\n",
      "ps_calc_08          int64\n",
      "ps_calc_09          int64\n",
      "ps_calc_10          int64\n",
      "ps_calc_11          int64\n",
      "ps_calc_12          int64\n",
      "ps_calc_13          int64\n",
      "ps_calc_14          int64\n",
      "ps_calc_15_bin      int64\n",
      "ps_calc_16_bin      int64\n",
      "ps_calc_17_bin      int64\n",
      "ps_calc_18_bin      int64\n",
      "ps_calc_19_bin      int64\n",
      "ps_calc_20_bin      int64\n",
      "dtype: object\n",
      "float64    20\n",
      "int64      39\n",
      "dtype: int64\n",
      "All df set missing values\n",
      "Series([], dtype: float64)\n",
      "\n",
      "\n",
      "Uniques\n",
      "124931\n",
      "uniques_in_id == df.shape[0]\n",
      "True\n",
      "\n",
      " sample_submission \n",
      "\n",
      "   id  target\n",
      "0   0  0.0364\n",
      "1   1  0.0364\n",
      "2   2  0.0364\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 892816 entries, 0 to 892815\n",
      "Data columns (total 2 columns):\n",
      "id        892816 non-null int64\n",
      "target    892816 non-null float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 13.6 MB\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "is_explore_data = 1\n",
    "if is_explore_data:\n",
    "    # Overview of train data\n",
    "    print('\\n TRAINING DATA:----------------------------------------------- \\n')\n",
    "    print(df.head(3))\n",
    "    print('\\n')\n",
    "    print(df.info())\n",
    "    print('\\n')\n",
    "    print(df.describe())\n",
    "    print('\\n')\n",
    "    print(df.dtypes)\n",
    "    print(df.get_dtype_counts())\n",
    "\n",
    "    # missing_values\n",
    "    print('All df set missing values')\n",
    "    porto_seguro_insur.missing_values_in_dataframe(df)\n",
    "\n",
    "    print('Uniques')\n",
    "    uniques_in_id = np.unique(df.id.values).shape[0]\n",
    "    print(uniques_in_id)\n",
    "    print('uniques_in_id == df.shape[0]')\n",
    "    print(uniques_in_id == df.shape[0])\n",
    "\n",
    "    # Overview of sample_submission format\n",
    "    print('\\n sample_submission \\n')\n",
    "    print(df_submission.head(3))\n",
    "    print('\\n')\n",
    "    print(df_submission.info())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 0.477945\n",
      "Training accuracy: 88.1%\n",
      "Loss at iteration 100: 0.191353\n",
      "Training accuracy: 95.4%\n",
      "Loss at iteration 200: 0.186203\n",
      "Training accuracy: 95.4%\n",
      "Loss at iteration 300: 0.182868\n",
      "Training accuracy: 95.4%\n",
      "Loss at iteration 400: 0.180391\n",
      "Training accuracy: 95.5%\n",
      "Loss at iteration 500: 0.178418\n",
      "Training accuracy: 95.5%\n",
      "Loss at iteration 600: 0.176696\n",
      "Training accuracy: 95.5%\n",
      "Loss at iteration 700: 0.175110\n",
      "Training accuracy: 95.5%\n",
      "Loss at iteration 800: 0.173571\n",
      "Training accuracy: 95.5%\n",
      "Test accuracy: 95.1%\n"
     ]
    }
   ],
   "source": [
    "is_prepare_data = 1\n",
    "if is_prepare_data:\n",
    "    df_test_num_features = porto_seguro_insur.extract_numerical_features(df_test)\n",
    "    df_y = df.loc[:, ['target']]\n",
    "    df = df.loc[:, df_test_num_features]\n",
    "\n",
    "is_prediction = 1\n",
    "if is_prediction:\n",
    "    # Subset the data to make it run faster\n",
    "    # (595212, 59)\n",
    "    # (892816, 58)\n",
    "    # After dropping NaN\n",
    "    # (124931, 59)\n",
    "    # (186567, 58)\n",
    "    subset_size = 20000\n",
    "\n",
    "    num_labels = np.unique(df_y.loc[:subset_size, 'target'].values).shape[0]\n",
    "    num_columns = df[(df.columns[(df.columns != 'target') & (df.columns != 'id')])].shape[1]\n",
    "    # Reformat datasets\n",
    "    x_train = df.loc[:subset_size, (df.columns[(df.columns != 'target') & (df.columns != 'id')])].values\n",
    "    y_train = df_y.loc[:subset_size, 'target'].values\n",
    "    # We only need to one-hot-encode our labels since otherwise they will not match the dimension of the\n",
    "    # logits in our later computation.\n",
    "    y_train = porto_seguro_insur.reformat_data(y_train, num_labels=num_labels)\n",
    "\n",
    "    # Todo: we need testdata with labels to benchmark test results.\n",
    "    # Hack. Use subset of training data (not used in training model) as test data, since it has a label/target value\n",
    "    # In case there are duplicates in training data it may imply that test results are too good, when using\n",
    "    # a subset of training data for test.\n",
    "    # Todo: do cross-validation instead of only one subset testing as below in x_val\n",
    "    # Validation data is a subset of training data.\n",
    "    x_val = df.loc[subset_size:2*subset_size, (df.columns[(df.columns != 'target') & (df.columns != 'id')])].values\n",
    "    y_val = df_y.loc[subset_size:2*subset_size, 'target'].values\n",
    "    y_val = porto_seguro_insur.reformat_data(y_val, num_labels=num_labels)\n",
    "    # Test data.\n",
    "    x_test = df_test.loc[:, (df_test.columns[(df_test.columns != 'id')])].values\n",
    "\n",
    "    # Todo: we need validation data with labels to perform crossvalidation while training and get a better result.\n",
    "\n",
    "    # Tensorflow uses a dataflow graph to represent your computations in terms of dependencies.\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Load training and test data into constants that are attached to the graph\n",
    "        tf_train = tf.constant(x_train)\n",
    "        tf_train_labels = tf.constant(y_train)\n",
    "        tf_val = tf.constant(x_val)\n",
    "        tf_test = tf.constant(x_test)\n",
    "\n",
    "        # As in a neural network the goal is to compute the cross-entropy D(S(w,x), L)\n",
    "        # x, input training data\n",
    "        # w_ij, are elements of the weight matrix\n",
    "        # L, labels or target values of the training data (classification problem)\n",
    "        # S(), is softmax function\n",
    "        # Do the Multinomial Logistic Classification\n",
    "        # step 1.\n",
    "        # Compute y from the linear model y = WX + b, where b is the bias and W is randomly chosen on\n",
    "        # Gaussian distribution and bias is set to zero. The result is called the logit.\n",
    "        # step 2.\n",
    "        # Compute the softmax function S(Y) which gives distribution\n",
    "        # step 3.\n",
    "        # Compute cross-entropy D(S, L) = - Sum_i L_i*log(S_i)\n",
    "        # step 4.\n",
    "        # Compute loss L = 1/N * D(S, L)\n",
    "        # step 5.\n",
    "        # Use gradient-descent to find minimum of loss wrt. w and b by minimizing L(w,b).\n",
    "        # Update your weight and bias until minimum of loss function is reached\n",
    "        # w_i -> w_i - alpha*delta_w L\n",
    "        # b -> b - alpha*delta_b L\n",
    "        # OBS. step 5 is faster optimized if you have transformed the data to have zero mean and equal variance\n",
    "        # mu(x_i) = 0\n",
    "        # sigma(x_i) = sigma(x_j)\n",
    "        # This transformation makes it a well conditioned problem.\n",
    "\n",
    "        # Make a 2-layer Neural network (count number of layers of adaptive weights) with num_columns nodes\n",
    "        # in hidden layer.\n",
    "        # Initialize weights on truncated normal distribution. Initialize biases to zero.\n",
    "        # For every input vector corresponding to one sample we have D features s.t.\n",
    "        # a_j = Sum_i^D w^(1)_ji x_i + w^(1)_j0 , where index j is the number of nodes in the first hidden layer\n",
    "        # and it runs j=1,...,M\n",
    "        # Vectorizing makes the notation more compact\n",
    "        #     | --- x_1 --- |\n",
    "        #     | --- x_2 --- |\n",
    "        # X = | --- ..  --- |\n",
    "        #     | --- x_N --- |\n",
    "        # where each x is now a sample vector of dimension (1 x D) and where N is the number of samples.\n",
    "        # Similarly, define a tiling of N weight matrices w,\n",
    "        #     | --- w --- |\n",
    "        #     | --- w --- |\n",
    "        # W = | --- ..--- |\n",
    "        #     | --- w --- |\n",
    "        # where each w is now a matrix of dimension (M x D)\n",
    "        # We now form the tensor product between W and X but we need to transpose X as x.T to get (M x D).(D x 1)\n",
    "        # multiplication,\n",
    "        #       |  w.(x_1.T) |\n",
    "        #       |  w.(x_2.T) |\n",
    "        # W.X = |  ..        |\n",
    "        #       |  w.(x_N.T) |\n",
    "        # with W.X having dimensions (M*N x 1).\n",
    "        # Additionally, define a tiling of N bias vectors b that each are of dimension (M x 1),\n",
    "        #     |  b  |\n",
    "        #     |  b  |\n",
    "        # B = |  .. |\n",
    "        #     |  b  |\n",
    "        # with B having dimensions (M*N x 1).\n",
    "        # Finally, the activation is a (M*N x 1) vector given as A = W.X + B.\n",
    "        # Next, this is passed to an activation function like a simoid and then inserted in second layer of the NN.\n",
    "        # Let Z = sigmoid(A)\n",
    "        # Let C be the activation of the second layer,\n",
    "        # C = W^(2).Z + B^(2)\n",
    "        # where W^(2) is the tiling N second layer weight matrices w^(2) each with dimension (K x M). K is the\n",
    "        # number of outputs in the classification. The dimension of C is (K x N).\n",
    "        # Lastly, apply the sigmoid function to get the predictions\n",
    "        # P = sigmoid( C )\n",
    "        # which has dimensions (K x N) and is as expected an output vector (K x 1) for every N samples in our\n",
    "        # dataset. The output (K x 1)-vector is in a one-hot-encoded form.\n",
    "\n",
    "        # Choose number of nodes > than number of features.\n",
    "        M_nodes = 2*x_train.shape[1]\n",
    "        weights_1_layer = tf.Variable(tf.truncated_normal([num_columns, M_nodes], dtype=np.float64))\n",
    "        biases_1_layer = tf.Variable(tf.zeros([M_nodes], dtype=np.float64))\n",
    "        weights_2_layer = tf.Variable(tf.truncated_normal([M_nodes, num_labels], dtype=np.float64))\n",
    "        biases_2_layer = tf.Variable(tf.zeros([num_labels], dtype=np.float64))\n",
    "\n",
    "        # Logits and loss function.\n",
    "        logits_hidden_1_layer = porto_seguro_insur.linear_model(tf_train, weights_1_layer, biases_1_layer)\n",
    "        # Output unit activations of first layer\n",
    "        a_1_layer = porto_seguro_insur.activation_hidden(logits_hidden_1_layer)\n",
    "        logits_2_layer = porto_seguro_insur.linear_model(a_1_layer, weights_2_layer, biases_2_layer)\n",
    "        switch_var = 0\n",
    "        if switch_var == 1:\n",
    "            loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,\n",
    "                                                                                   logits=logits_2_layer))\n",
    "        else:\n",
    "            loss_function = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_train_labels,\n",
    "                                                                                   logits=logits_2_layer))\n",
    "\n",
    "        # Find minimum of loss function using gradient-descent.\n",
    "        optimized_weights_and_bias = tf.train.GradientDescentOptimizer(0.5).minimize(loss=loss_function)\n",
    "\n",
    "        # Accuracy variables using the initial values for weights and bias of our linear model.\n",
    "        train_prediction = porto_seguro_insur.activation_out(logits_2_layer)\n",
    "        # Applying optimized weights and bias to validation data\n",
    "        logits_hidden_1_layer_val = porto_seguro_insur.linear_model(tf_val, weights_1_layer, biases_1_layer)\n",
    "        a_1_layer_val = porto_seguro_insur.activation_hidden(logits_hidden_1_layer_val)\n",
    "        logits_2_layer_val = porto_seguro_insur.linear_model(a_1_layer_val, weights_2_layer, biases_2_layer)\n",
    "        val_prediction = porto_seguro_insur.activation_out(logits_2_layer_val)\n",
    "\n",
    "        # Applying optimized weights and bias to test data\n",
    "        logits_hidden_1_layer_test = porto_seguro_insur.linear_model(tf_test, weights_1_layer, biases_1_layer)\n",
    "        a_1_layer_test = porto_seguro_insur.activation_hidden(logits_hidden_1_layer_test)\n",
    "        logits_2_layer_test = porto_seguro_insur.linear_model(a_1_layer_test, weights_2_layer, biases_2_layer)\n",
    "        test_prediction = porto_seguro_insur.activation_out(logits_2_layer_test)\n",
    "\n",
    "    number_of_iterations = 900\n",
    "    # Creating a tensorflow session to effeciently run same computation multiple times using definitions in defined\n",
    "    # dataflow graph.\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Ensure that variables are initialized as done in our graph defining the dataflow.\n",
    "        tf.global_variables_initializer().run()\n",
    "        for ite in range(number_of_iterations):\n",
    "            # Compute loss and predictions\n",
    "            loss, predictions = session.run([optimized_weights_and_bias, loss_function, train_prediction])[1:3]\n",
    "            if ite % 100 == 0:\n",
    "                print('Loss at iteration %d: %f' % (ite, loss))\n",
    "                print('Training accuracy: %.1f%%' % porto_seguro_insur.accuracy(predictions, y_train))\n",
    "        print('Test accuracy: %.1f%%' % porto_seguro_insur.accuracy(val_prediction.eval(), y_val))\n",
    "        output = test_prediction.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20171027_10h17m32s\n"
     ]
    }
   ],
   "source": [
    "is_make_prediction = 1\n",
    "if is_make_prediction:\n",
    "    ''' Submission '''\n",
    "    # Submission requires a csv file with id and target columns.\n",
    "    submission = pd.DataFrame({'id': id_df_test, 'target': np.argmax(output, 1)})\n",
    "    submission.to_csv(''.join(['submission_porto_seguro_insur_', porto_seguro_insur.timestamp, '.csv']), index=False)\n",
    "    print(porto_seguro_insur.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
